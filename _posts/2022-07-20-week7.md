---
layout: post
title: Week 8
---

## Research results check in pt. 1 ##

We've been waiting for participants responses to roll in and we finally have enough data to start sorting it. At the moment we have around 2000+ responses. We've been using a shared Google Drive to store and share the preprocessed and processed data.

For the purposes of determining useful responses from spam or otherwise low quality responses I split the responses into the following categories:

* Substantial Open Answer - provided a text answer to multiple open answer questions
* Some Open Answer - provided at least one answer to open answer question though it's likely short
* Low Quality Answer - gave conflicting or seemingly nonsense answers ex. Yes to the code problem having issues but not explaining the issue in the following answer areas.
* Suspected Bot or Copy - lot of the same specific or very similar answers in multiple fields, had 0 clicks on code review challenges, lots of the same IP addresses in a short amount of time

Often entries came down to a matter of opinion, and I recognized that I might be missing some context from
the smart contract developers' viewpoint. I discussed these unfamiliar terms with Tanusree and while some still were decided to be spam some were references to smart contract software or open source products that I added to my lexicon for the project.

After sorting and highlighting answers of note I uploaded my copy of the spreadsheet to the shared Drive. I'm not waiting for more responses to build to continue the process.
